#!/usr/bin/env python3
"""
Open Interpreter + Ollama Demo
‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Open Interpreter ‡∏Å‡∏±‡∏ö Ollama Local Models
"""

from interpreter import interpreter
import os
import requests

def check_ollama_status():
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ Ollama server"""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get('models', [])
            print(f"‚úÖ Ollama server ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà - ‡∏û‡∏ö {len(models)} models")
            return True, models
        else:
            print("‚ùå Ollama server ‡πÑ‡∏°‡πà‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á")
            return False, []
    except requests.exceptions.ConnectionError:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Ollama server")
        print("üí° ‡∏•‡∏≠‡∏á‡∏£‡∏±‡∏ô: ollama serve")
        return False, []
    except Exception as e:
        print(f"‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
        return False, []

def setup_ollama_interpreter(model_name="llama3.2:latest"):
    """‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Open Interpreter ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ Ollama"""
    print(f"üîß ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Open Interpreter ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ {model_name}")
    
    # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Ollama
    interpreter.llm.model = f"ollama/{model_name}"
    interpreter.llm.api_base = "http://localhost:11434"
    interpreter.llm.temperature = 0.3
    interpreter.auto_run = True
    interpreter.offline = True
    
    print(f"‚úÖ Model: {interpreter.llm.model}")
    print(f"‚úÖ API Base: {interpreter.llm.api_base}")
    print(f"‚úÖ Temperature: {interpreter.llm.temperature}")
    print(f"‚úÖ Auto run: {interpreter.auto_run}")

def demo_simple_conversation():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô"""
    print("\nüí¨ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô")
    print("=" * 40)
    
    try:
        print("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ñ‡∏≤‡∏° AI...")
        response = interpreter.chat("‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ! ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ï‡∏±‡∏ß‡∏™‡∏±‡πâ‡∏ô‡πÜ ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢", display=False)
        print("‚úÖ ‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
    except Exception as e:
        print(f"‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")

def demo_code_execution():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î"""
    print("\nüêç ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î Python")
    print("=" * 40)
    
    try:
        print("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏´‡πâ AI ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î...")
        response = interpreter.chat("""
        ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î Python ‡∏ó‡∏µ‡πà:
        1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç 1-10
        2. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ú‡∏•‡∏£‡∏ß‡∏°
        3. ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        """, display=False)
        print("‚úÖ ‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
    except Exception as e:
        print(f"‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")

def demo_file_operations():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Å‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå"""
    print("\nüìÅ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Å‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå")
    print("=" * 40)
    
    try:
        print("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏´‡πâ AI ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå...")
        response = interpreter.chat("""
        ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏∑‡πà‡∏≠ 'ollama_test.txt' ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°:
        '‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏à‡∏≤‡∏Å Open Interpreter + Ollama!'
        ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡πÅ‡∏™‡∏î‡∏á
        """, display=False)
        print("‚úÖ ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Å‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
    except Exception as e:
        print(f"‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")

def demo_system_info():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏ö‡∏ö"""
    print("\nüñ•Ô∏è  ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏ö‡∏ö")
    print("=" * 40)
    
    try:
        print("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏´‡πâ AI ‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏ö‡∏ö...")
        response = interpreter.chat("""
        ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏ö‡∏ö‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô:
        1. ‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£
        2. ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        3. ‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        """, display=False)
        print("‚úÖ ‡∏Å‡∏≤‡∏£‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
    except Exception as e:
        print(f"‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")

def list_available_models():
    """‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ models ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô Ollama"""
    print("\nüìã ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ Ollama Models ‡∏ó‡∏µ‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:")
    print("=" * 50)
    
    try:
        response = requests.get("http://localhost:11434/api/tags")
        if response.status_code == 200:
            models = response.json().get('models', [])
            for i, model in enumerate(models[:10], 1):  # ‡πÅ‡∏™‡∏î‡∏á 10 ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å
                name = model.get('name', 'Unknown')
                size = model.get('size', 0) / (1024**3)  # Convert to GB
                print(f"{i:2d}. {name:<25} ({size:.1f} GB)")
            
            if len(models) > 10:
                print(f"    ... ‡πÅ‡∏•‡∏∞‡∏≠‡∏µ‡∏Å {len(models) - 10} models")
        else:
            print("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ models ‡πÑ‡∏î‡πâ")
    except Exception as e:
        print(f"‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")

def main():
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å"""
    print("üéØ Open Interpreter + Ollama Demo")
    print("=" * 60)
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Ollama
    is_running, models = check_ollama_status()
    
    if not is_running:
        print("\nüí° ‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏£‡∏¥‡πà‡∏° Ollama:")
        print("1. ‡πÄ‡∏õ‡∏¥‡∏î terminal ‡πÉ‡∏´‡∏°‡πà")
        print("2. ‡∏£‡∏±‡∏ô: ollama serve")
        print("3. ‡∏£‡∏±‡∏ô‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏ô‡∏µ‡πâ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á")
        return
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ models
    list_available_models()
    
    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å model (‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏î‡πâ)
    available_models = [
        "llama3.2:latest",
        "qwen2.5:latest", 
        "phi3:latest",
        "codestral:22b",
        "deepseek-coder:33b"
    ]
    
    print(f"\nü§ñ ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Model ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö:")
    for i, model in enumerate(available_models, 1):
        print(f"{i}. {model}")
    
    try:
        choice = input("\n‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç model (‡∏´‡∏£‡∏∑‡∏≠ Enter ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö llama3.2): ").strip()
        if choice and choice.isdigit() and 1 <= int(choice) <= len(available_models):
            selected_model = available_models[int(choice) - 1]
        else:
            selected_model = "llama3.2:latest"
    except:
        selected_model = "llama3.2:latest"
    
    print(f"\nüöÄ ‡πÉ‡∏ä‡πâ model: {selected_model}")
    
    # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ interpreter
    setup_ollama_interpreter(selected_model)
    
    # ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö
    print("\n" + "="*60)
    print("üß™ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö...")
    
    demo_simple_conversation()
    demo_code_execution()
    demo_file_operations()
    demo_system_info()
    
    print("\n" + "="*60)
    print("‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö Open Interpreter + Ollama!")
    print("üí° ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Open Interpreter ‡∏Å‡∏±‡∏ö Ollama ‡πÅ‡∏•‡πâ‡∏ß")
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠
    print(f"\nüîß ‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠:")
    print(f"interpreter --model ollama/{selected_model}")
    print(f"interpreter --api-base http://localhost:11434")

if __name__ == "__main__":
    main()
